{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup "
      ],
      "metadata": {
        "id": "TD7seO22cOo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G0oqLyCebwZR"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Activation, BatchNormalization, Conv2D, MaxPooling2D, Input, Dropout, GlobalAveragePooling2D,concatenate\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import PIL.ImageOps \n",
        "import csv\n",
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "from natsort import natsorted\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders --quiet"
      ],
      "metadata": {
        "id": "zvJuMmdmdIjY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount to Google Drive folder. Make sure the folder `malaria_project` is inside `MyDrive`. If it is in `shared with me` right click and make a shortcut to `MyDrive`. "
      ],
      "metadata": {
        "id": "kmsm-rP-cRLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMitYht2b_FW",
        "outputId": "74952aa2-2e86-4cf7-c363-7902f5a9b281"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables:"
      ],
      "metadata": {
        "id": "JTA6Wnw8gob2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "Fw0Ax-dVgpbD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the satellite data\n",
        "First we split the satellite images into `test`,`train`, and `validation` folders"
      ],
      "metadata": {
        "id": "ttLY2JA6dL68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Change it such that the entire satellite folder is within malaria_project"
      ],
      "metadata": {
        "id": "HbwTloWkdptC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(\"/content/drive/MyDrive/x_data/x2\", output=\"output\",\n",
        "    seed=1337, ratio=(.7, .2, .1), group_prefix=None, move=False) # (train:validation:test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN2NstqCb_pL",
        "outputId": "d3050f88-29cd-40f2-bb8b-3211436e3cdb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 5133 files [07:03, 12.11 files/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are going to use `DataFrames` to our `ImageDataGenerators` we need to create them such that the entire filename is the `x_col` entry, and we can access the label. It is required that the name of the entry in `x_col` has the exact same name as the satellite image in order to match them."
      ],
      "metadata": {
        "id": "WLARtQEVdxLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function is responsible for creating the dataframe containing the labels and image data\n",
        "'''\n",
        "def createDataFrame(label_dir, data_dir, ending=True):\n",
        "  #Retrieve the filenames and ids.\n",
        "  filenames = []\n",
        "  ids = []\n",
        "  for file in os.listdir(data_dir):\n",
        "    if file == \".DS_Store\":\n",
        "      continue\n",
        "    filenames.append(file)\n",
        "    if ending:\n",
        "      ids.append(file.split(\"_\")[0])\n",
        "    else:\n",
        "      ids.append(file.split(\".\")[0])\n",
        "  #Create empty DataFrame\n",
        "  df = pd.DataFrame()\n",
        "  #Append the data\n",
        "  df['filename'] = filenames\n",
        "  df['id'] = ids\n",
        "\n",
        "  #Append the labels\n",
        "  start_list = []\n",
        "  with open(label_dir) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for i,line in enumerate(csv_reader):\n",
        "      #print(line[0])\n",
        "      id = line[0]\n",
        "      pr = line[4]\n",
        "      for img in filenames:\n",
        "        if ending:\n",
        "          img_id = img.split('_')[0]\n",
        "        else:\n",
        "          img_id = img.split(\".\")[0]\n",
        "          \n",
        "        if img_id == id:\n",
        "          index = np.where(df[\"filename\"] == img)\n",
        "          start_list.append(float(pr))\n",
        "    df['label'] = start_list\n",
        "  return df.sort_values(by='id')"
      ],
      "metadata": {
        "id": "YZrycCfqdxrq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the `DataFrames`"
      ],
      "metadata": {
        "id": "62P2Oh4zeq2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = 'output/train/satellite_data_v2'\n",
        "test_data_dir = 'output/test/satellite_data_v2'\n",
        "val_data_dir = 'output/val/satellite_data_v2'\n",
        "label_dir = '/content/drive/MyDrive/malaria_project/long_lat_year_with_confidential_from2010to18_size10orGreater.csv'\n",
        "train_df = createDataFrame(label_dir=label_dir, data_dir=train_data_dir)\n",
        "test_df = createDataFrame(label_dir=label_dir, data_dir=test_data_dir)\n",
        "val_df = createDataFrame(label_dir=label_dir, data_dir=val_data_dir)"
      ],
      "metadata": {
        "id": "cMMMPQ46eocT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the landcover data"
      ],
      "metadata": {
        "id": "UBBv_FFye2b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Finding the names of the files that needs to be copied'''\n",
        "x_names = []\n",
        "for file_name in os.listdir(\"/content/drive/MyDrive/x_data/x2/satellite_data_v2\"):\n",
        "  x_names.append(file_name.split(\"_\")[0])"
      ],
      "metadata": {
        "id": "7C_O749heuqn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Loops through the images and copies the corresponding land cover'''\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = r\"/content/drive/MyDrive/malaria_project/landcover_with_confi_2010_2018_size10andGreater\"\n",
        "if not os.path.exists('landcover_full'):\n",
        "   os.makedirs('landcover_full')\n",
        "destination_folder = r\"landcover_full\"\n",
        "\n",
        "for file_name in os.listdir(source_folder):\n",
        "  for x_name in x_names:\n",
        "    if file_name.split(\".\")[0] == x_name:\n",
        "      source = source_folder + \"/\"+ file_name\n",
        "      destination = destination_folder + \"/\" + file_name\n",
        "      shutil.copy(source, destination)"
      ],
      "metadata": {
        "id": "oHL8-wQlfRGo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Fix such that the landcover images has the correct size"
      ],
      "metadata": {
        "id": "4Fla6dTrfgqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#crop the landcovers\n",
        "for i,image in enumerate(os.listdir(\"landcover_full\")):\n",
        "    if image == \".DS_Store\" or image ==\".ipynb_checkpoints\":\n",
        "        continue\n",
        "    img = Image.open(\"landcover_full/\"+image)\n",
        "    if img.size != (8, 8):\n",
        "      img = img.resize((8,8))\n",
        "      img.save('landcover_full/'+image, 'TIFF')\n"
      ],
      "metadata": {
        "id": "I7hi6H26fk_X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the images locally we can split it into folders. First we create all the directories and then we copy the images into the correct ones, according to the satellite data. "
      ],
      "metadata": {
        "id": "af2HHpb8f0WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the directories\n",
        "if not os.path.exists('output_landcover'):\n",
        "   os.makedirs('output_landcover')\n",
        "#testing\n",
        "if not os.path.exists('output_landcover/test'):\n",
        "   os.makedirs('output_landcover/test')\n",
        "if not os.path.exists('output_landcover/test/landcover'):\n",
        "   os.makedirs('output_landcover/test/landcover')\n",
        "#training\n",
        "if not os.path.exists('output_landcover/train'):\n",
        "   os.makedirs('output_landcover/train')\n",
        "if not os.path.exists('output_landcover/train/landcover'):\n",
        "   os.makedirs('output_landcover/train/landcover')\n",
        "#validation\n",
        "if not os.path.exists('output_landcover/val'):\n",
        "   os.makedirs('output_landcover/val')\n",
        "if not os.path.exists('output_landcover/val/landcover'):\n",
        "   os.makedirs('output_landcover/val/landcover')"
      ],
      "metadata": {
        "id": "mKzhLVD7f1lo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Copies the files from the landcover directory to the correct splitted folders'''\n",
        "def copyfiles(dir, src_dir, dest_dir):\n",
        "  for file_name in os.listdir(dir):\n",
        "    file_name_no_coords = file_name.split(\"_\")[0]\n",
        "    source = src_dir + file_name_no_coords + \".tiff\"\n",
        "    destination = dest_dir + file_name_no_coords + \".tiff\"\n",
        "    shutil.copy(source, destination)\n",
        "  \n",
        "copyfiles(dir=\"output/test/satellite_data_v2\", src_dir=\"landcover_full/\", dest_dir=\"output_landcover/test/landcover/\")\n",
        "copyfiles(dir=\"output/train/satellite_data_v2\", src_dir=\"landcover_full/\", dest_dir=\"output_landcover/train/landcover/\")\n",
        "copyfiles(dir=\"output/val/satellite_data_v2\", src_dir=\"landcover_full/\", dest_dir=\"output_landcover/val/landcover/\")"
      ],
      "metadata": {
        "id": "XMqp-SVrgDPz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly we can create the `DataFrames`"
      ],
      "metadata": {
        "id": "0zK6LaligXrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir_landcover = 'output_landcover/train/landcover'\n",
        "test_data_dir_landcover = 'output_landcover/test/landcover'\n",
        "val_data_dir_landcover = 'output_landcover/val/landcover'\n",
        "label_dir_landcover = '/content/drive/MyDrive/malaria_project/long_lat_year_with_confidential_from2010to18_size10orGreater.csv'\n",
        "train_landcover_df = createDataFrame(label_dir=label_dir_landcover, data_dir=train_data_dir_landcover, ending=False)\n",
        "test_landcover_df = createDataFrame(label_dir=label_dir_landcover, data_dir=test_data_dir_landcover, ending=False)\n",
        "val_landcover_df = createDataFrame(label_dir=label_dir_landcover, data_dir=val_data_dir_landcover, ending=False)"
      ],
      "metadata": {
        "id": "aMmYuLq_gHtK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the `ImageDataGenerators`"
      ],
      "metadata": {
        "id": "YQi8u9nHgc6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Satellite\n",
        "imgen_sat = ImageDataGenerator()\n",
        "train_generator_sat = imgen_sat.flow_from_dataframe(dataframe=train_df, \n",
        "                                            directory=train_data_dir, \n",
        "                                            x_col=\"filename\", \n",
        "                                            y_col=\"label\", \n",
        "                                            has_ext=True,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=BATCH_SIZE, \n",
        "                                            class_mode=\"other\", \n",
        "                                            target_size=(1024,1024))\n",
        "#valgen_sat = ImageDataGenerator()\n",
        "#val_generator_sat = valgen_sat.flow_from_dataframe(dataframe=val_d)\n",
        "\n",
        "\n",
        "#Landcover\n",
        "imgen_lc = ImageDataGenerator()\n",
        "train_generator_lc = imgen_lc.flow_from_dataframe(dataframe=train_landcover_df, \n",
        "                                            directory=train_data_dir_landcover, \n",
        "                                            x_col=\"filename\", \n",
        "                                            y_col=\"label\", \n",
        "                                            has_ext=True,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=BATCH_SIZE, \n",
        "                                            class_mode=\"other\", \n",
        "                                            target_size=(8,8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "temPC28GghxA",
        "outputId": "27cbafed-c257-4bf4-bbdf-029cf305af06"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3593 validated image filenames.\n",
            "Found 3593 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JoinedGen(tensorflow.keras.utils.Sequence):\n",
        "    '''\n",
        "    This clas is reponsible for manipulating the outputs of the generators \n",
        "    to have the correct form which our model can take as input. It takes two \n",
        "    ImageDataGenerator instances as input.\n",
        "\n",
        "    Args: \n",
        "        input_gen1: An ImageDataGenerator instance.\n",
        "        input_gen2: An ImageDataGenerator instance\n",
        "    '''\n",
        "    def __init__(self, input_gen1, input_gen2):\n",
        "        self.gen1 = input_gen1\n",
        "        self.gen2 = input_gen2\n",
        "\n",
        "        assert len(input_gen1) == len(input_gen2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gen1)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x1,y1 = self.gen1[i]\n",
        "        x2,y2 = self.gen2[i]\n",
        "\n",
        "        return [x1,x2],y1 #as the labels are always the same, we can simply output the first.\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.gen1.on_epoch_end()\n",
        "        self.gen2.on_epoch_end()\n",
        "        self.gen2.index_array = self.gen1.index_array #responsible for shuffling\n"
      ],
      "metadata": {
        "id": "qzyMt5wchBpb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the model"
      ],
      "metadata": {
        "id": "Ois2rJStg1oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Toy model'''\n",
        "sat_input = keras.Input(shape=(1024,1024,3))\n",
        "lc_input = keras.Input(shape=(8,8,3))\n",
        "\n",
        "x = MaxPooling2D((128,128))(sat_input)\n",
        "x = concatenate([x,lc_input])\n",
        "x = Flatten()(x)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        "new_simple_model = Model(inputs=[sat_input, lc_input], outputs=x) \n",
        "new_simple_model.compile(optimizer='adam', loss='mse')\n",
        "summary = new_simple_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ToDzX3gzFZ",
        "outputId": "efac1046-c869-4086-cccd-9482f38c670f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 1024, 1024,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 3)     0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 8, 8, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 8, 8, 6)      0           ['max_pooling2d_1[0][0]',        \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 384)          0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            385         ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 385\n",
            "Trainable params: 385\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_gen = JoinedGen(train_generator_sat, train_generator_lc)\n",
        "vgg_history = new_simple_model.fit(my_gen,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            epochs=EPOCHS,\n",
        "                            steps_per_epoch=train_generator_sat.samples//BATCH_SIZE,\n",
        "                            #validation_data=val_generator,\n",
        "                            #,\n",
        "                            #validation_steps=n_val_steps,\n",
        "                            #callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "emUjER-Pg_FM",
        "outputId": "7398bbb7-1c64-4f4e-cfe5-45af7ac676dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 75/112 [===================>..........] - ETA: 1:51 - loss: 8903.1094"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8c9425ff8c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJoinedGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator_sat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator_lc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m vgg_history = new_simple_model.fit(my_gen,\n\u001b[0m\u001b[1;32m      3\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator_sat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_D__CIB4imsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}